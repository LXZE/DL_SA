{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is CACB-8726\n",
      "\n",
      " Directory of C:\\Users\\L\\Desktop\\work\\dataset\n",
      "\n",
      "02/01/2019  08:55 AM    <DIR>          .\n",
      "02/01/2019  08:55 AM    <DIR>          ..\n",
      "02/01/2019  12:34 AM         4,124,645 neg.txt\n",
      "02/01/2019  08:52 AM         7,948,509 new_bilstm_neg_tok.pkl\n",
      "02/01/2019  08:52 AM        19,913,070 new_bilstm_pos_tok.pkl\n",
      "02/01/2019  01:45 AM         7,722,752 new_neg_tok.pkl\n",
      "02/01/2019  01:45 AM        19,505,617 new_pos_tok.pkl\n",
      "02/01/2019  12:34 AM        10,450,593 pos.txt\n",
      "02/01/2019  12:57 AM        15,330,064 vc_redist.x64.exe\n",
      "11/08/2018  12:30 AM    <DIR>          wongnai\n",
      "11/08/2018  12:30 AM           629,380 wongnai_1.csv\n",
      "11/08/2018  12:30 AM         3,131,209 wongnai_2.csv\n",
      "11/08/2018  12:30 AM        17,993,664 wongnai_3.csv\n",
      "11/08/2018  12:30 AM        10,423,335 wongnai_5.csv\n",
      "              11 File(s)    117,172,838 bytes\n",
      "               3 Dir(s)  25,067,458,560 bytes free\n",
      " Volume in drive C has no label.\n",
      " Volume Serial Number is CACB-8726\n",
      "\n",
      " Directory of C:\\Users\\L\\Desktop\\work\\model\n",
      "\n",
      "01/01/2019  11:20 PM    <DIR>          .\n",
      "01/01/2019  11:20 PM    <DIR>          ..\n",
      "11/10/2018  02:30 AM        73,858,744 model.h5\n",
      "11/10/2018  02:35 AM             3,439 model.json\n",
      "11/14/2018  01:26 AM        75,898,928 model_attn.h5\n",
      "11/14/2018  01:26 AM             3,691 model_attn.json\n",
      "               4 File(s)    149,764,802 bytes\n",
      "               2 Dir(s)  25,067,474,944 bytes free\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.name == 'nt':\n",
    "    !dir \"../dataset\"\n",
    "    !dir \"../model\"\n",
    "else:\n",
    "    !dir \"../dataset\"\n",
    "    !dir \"../model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "\n",
    "def plot_accnloss(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)    \n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    tick_marks = [0, 1]\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.grid(False)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "#     plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "get_val = lambda arr: list(map(lambda x: x[1], arr))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9410, 600) (9410, 2)\n",
      "float32 int32\n",
      "(1661, 600) (1661, 2)\n",
      "float32 int32\n",
      "pos size = 1112\n",
      "neg size = 549\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "from attention import AttentionWithContext\n",
    "\n",
    "import time, sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Activation, Conv1D, MaxPooling1D, Dropout, Flatten, Bidirectional, GRU, Embedding, LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import optimizers, utils\n",
    "from keras import backend as K\n",
    "\n",
    "import dill as pickle\n",
    "\n",
    "\n",
    "data_dim = 300\n",
    "num_classes = 2\n",
    "\n",
    "input_len = 600\n",
    "[x_train, x_test, y_train, y_test] = np.load('../dataset/train_test_data.npy')\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_train.dtype, y_train.dtype)\n",
    "print(x_test.shape, y_test.shape)\n",
    "print(x_test.dtype, y_test.dtype)\n",
    "\n",
    "print(f'pos size = { len(y_test[np.where( y_test[:,0] == 0 )]) }')\n",
    "print(f'neg size = { len(y_test[np.where( y_test[:,0] == 1 )]) }')\n",
    "\n",
    "word_vec = np.load('../model/vec.npy')\n",
    "\n",
    "print(K.floatx()) # should equal float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 600, 300)          19500000  \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 600, 300)          541200    \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 600, 256)          439296    \n",
      "_________________________________________________________________\n",
      "attention_with_context_5 (At (None, 256)               66048     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 20,579,698\n",
      "Trainable params: 20,579,698\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dropout = 0.2\n",
    "dense_dropout = 0.2\n",
    "rnn_dropout = 0.1\n",
    "learning_rate = 1e-3\n",
    "batch_size = 50\n",
    "epochs = 5\n",
    "\n",
    "kernel_size = 3\n",
    "pool_size = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_vec), 300, input_length=input_len, weights=[word_vec], trainable=True))\n",
    "\n",
    "# cnn\n",
    "# model.add(Conv1D(250, kernel_size, padding='same', activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=pool_size))\n",
    "# model.add(Conv1D(200, kernel_size, padding='same', activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "# 2blstm\n",
    "model.add(Bidirectional(LSTM(150, return_sequences=True, dropout=dropout, recurrent_dropout=rnn_dropout)))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True, dropout=dropout, recurrent_dropout=rnn_dropout)))\n",
    "# model.add(Bidirectional(LSTM(128, dropout=dropout, recurrent_dropout=rnn_dropout)))\n",
    "\n",
    "# 2bgru\n",
    "# model.add(Bidirectional(GRU(150, return_sequences=True, dropout=dropout, recurrent_dropout=rnn_dropout)))\n",
    "# model.add(Bidirectional(GRU(128, return_sequences=True, dropout=dropout, recurrent_dropout=rnn_dropout)))\n",
    "# model.add(Bidirectional(GRU(128, dropout=dropout, recurrent_dropout=rnn_dropout)))\n",
    "\n",
    "# attn (require return_sequences=True)\n",
    "model.add(AttentionWithContext())\n",
    "\n",
    "model.add(Dense(128, activation='tanh'))\n",
    "model.add(Dropout(dense_dropout))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "adam = optimizers.Adam(lr=learning_rate)\n",
    "# model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9410 samples, validate on 1661 samples\n",
      "Epoch 1/5\n",
      "9410/9410 [==============================] - 631s 67ms/step - loss: 0.5579 - acc: 0.7320 - val_loss: 0.4003 - val_acc: 0.8224\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.82240, saving model to ../model/result/20190207_173135/model_20190207_173135.hdf5\n",
      "Epoch 2/5\n",
      "9410/9410 [==============================] - 631s 67ms/step - loss: 0.3489 - acc: 0.8494 - val_loss: 0.2183 - val_acc: 0.9127\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.82240 to 0.91270, saving model to ../model/result/20190207_173135/model_20190207_173135.hdf5\n",
      "Epoch 3/5\n",
      "6300/9410 [===================>..........] - ETA: 3:15 - loss: 0.2093 - acc: 0.9194"
     ]
    }
   ],
   "source": [
    "drive_path = '../model/result/'\n",
    "date = time.strftime('%Y%m%d_%H%M%S')\n",
    "pathlib.Path(f'{drive_path}{date}').mkdir(parents=True, exist_ok=True)\n",
    "mon = 'val_acc'\n",
    "mode_mon = 'max'\n",
    "file_path = f'{drive_path}{date}/model_{date}.hdf5'\n",
    "ckpt = ModelCheckpoint(file_path, monitor=mon, verbose=1, save_best_only=True, mode=mode_mon)\n",
    "early = EarlyStopping(monitor=mon, mode=mode_mon, patience=5)\n",
    "\n",
    "histories = []\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), callbacks=[ckpt, early])\n",
    "del history.model\n",
    "histories.append(history)\n",
    "\n",
    "pickle.dump(histories, open(f'{drive_path}{date}/model_{date}_fitting_history.pkl', 'wb'))\n",
    "        \n",
    "model_name = f'{drive_path}{date}/model_{date}.h5'\n",
    "model.save_weights(model_name)\n",
    "model_json = model.to_json()\n",
    "\n",
    "with open(f'{drive_path}{date}/model_{date}.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "\n",
    "model = load_model(file_path, custom_objects={'AttentionWithContext': AttentionWithContext})\n",
    "print(\"Load best model from disk\")\n",
    "\n",
    "rand = np.random.choice(len(x_test), 10, replace=False)\n",
    "for i in rand:\n",
    "\tpre_x, pre_y = x_test[i], y_test[i]\n",
    "\tpre_x = pre_x.reshape(1, input_len)\n",
    "\tres_y = model.predict_classes(pre_x, batch_size=1)\n",
    "\tprint('Prediction: {}'.format(res_y[0]))\n",
    "\tprint('Result: {}'.format(str(np.where(pre_y==1.)[0])[1]))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print(\"{}: {}\".format(model.metrics_names[0], score[0]))\n",
    "print(\"{}: {}%\".format(model.metrics_names[1], score[1]*100))\n",
    "\n",
    "y_true = get_val(y_test)\n",
    "y_predict = model.predict_classes(x_test) \n",
    "\n",
    "print(metrics.classification_report(y_true, y_predict, target_names = ['negative', 'positive'], digits=6))\n",
    "\n",
    "with open(f'{drive_path}{date}/model_{date}_result.txt', 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "    f.write('{}: {}\\n'.format(model.metrics_names[0], score[0]))\n",
    "    f.write('{}: {}%\\n'.format(model.metrics_names[1], score[1]*100))\n",
    "    f.write('{}: {}%\\n'.format('Report', metrics.classification_report(y_true, y_predict, target_names = ['negative', 'positive'], digits=6)))\n",
    "    \n",
    "    \n",
    "plot_accnloss(history)\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_true, y_predict, labels=[1,0]), \n",
    "                      classes=['positive', 'negative'], \n",
    "                      normalize=True,\n",
    "                      title='Confusion matrix')\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_true, y_predict, labels=[1,0]), \n",
    "                      classes=['positive', 'negative'], \n",
    "                      normalize=False,\n",
    "                      title='Confusion matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
